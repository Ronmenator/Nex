// GPT-2 with Multi-Query Attention — Nex + libtorch
//
// Architecture (Pre-LayerNorm GPT-2 with MQA):
//   Token Embedding + Position Embedding
//   → N × [ LN → MQA(4 heads, shared K/V) + Residual → LN → FFN(GELU) + Residual ]
//   → Final LN → LM Head
//
// Usage:
//   nex run examples/gpt2_transformer [--epochs N] [--lr F] [--seed N] [--save PATH]

import torch.tensor
import torch.nn
import torch.loss
import torch.optim
import torch.model

// ===== Config =====

struct GPTConfig {
    n_embd: Int
    n_head: Int
    n_layer: Int
    vocab_size: Int
    block_size: Int
    d_head: Int
    d_ff: Int
}

// ===== Builder helpers =====

def build_proj(d_in: Int, d_out: Int) -> Module {
    var m: Module = nn_sequential_new()
    nn_linear(m, d_in, d_out)
    return m
}

def build_ffn(d_model: Int, d_ff: Int) -> Module {
    var m: Module = nn_sequential_new()
    nn_linear(m, d_model, d_ff)
    nn_gelu(m)
    nn_linear(m, d_ff, d_model)
    nn_dropout(m, 0.1)
    return m
}

def build_ln(dim: Int) -> Module {
    var m: Module = nn_sequential_new()
    nn_layer_norm(m, dim)
    return m
}

def build_embedding(num_emb: Int, emb_dim: Int) -> Module {
    var m: Module = nn_sequential_new()
    nn_embedding(m, num_emb, emb_dim)
    return m
}

// ===== Attention helpers =====

def attn_head(q: Tensor, k: Tensor, v: Tensor, causal_mask: Tensor) -> Tensor {
    var kt: Tensor = k.transpose(0, 1)
    var scores: Tensor = q.matmul(kt).div_scalar(4.0)
    scores = scores.masked_fill(causal_mask, -100000.0)
    var weights: Tensor = scores.softmax(1)
    return weights.matmul(v)
}

def mqa_forward(
    q_proj: Module, k_proj: Module, v_proj: Module, o_proj: Module,
    x: Tensor, causal_mask: Tensor
) -> Tensor {
    var q_all: Tensor = q_proj.forward(x)
    var k: Tensor = k_proj.forward(x)
    var v: Tensor = v_proj.forward(x)

    var q0: Tensor = q_all.narrow(1, 0, 16)
    var q1: Tensor = q_all.narrow(1, 16, 16)
    var q2: Tensor = q_all.narrow(1, 32, 16)
    var q3: Tensor = q_all.narrow(1, 48, 16)

    var h0: Tensor = attn_head(q0, k, v, causal_mask)
    var h1: Tensor = attn_head(q1, k, v, causal_mask)
    var h2: Tensor = attn_head(q2, k, v, causal_mask)
    var h3: Tensor = attn_head(q3, k, v, causal_mask)

    var ctx: Tensor = h0.cat(h1, 1).cat(h2, 1).cat(h3, 1)
    return o_proj.forward(ctx)
}

def block_forward(
    ln1: Module, q_proj: Module, k_proj: Module, v_proj: Module,
    o_proj: Module, ln2: Module, ffn: Module,
    x: Tensor, causal_mask: Tensor
) -> Tensor {
    var normed: Tensor = ln1.forward(x)
    var attn_out: Tensor = mqa_forward(q_proj, k_proj, v_proj, o_proj, normed, causal_mask)
    x = x + attn_out

    normed = ln2.forward(x)
    var ff_out: Tensor = ffn.forward(normed)
    x = x + ff_out
    return x
}

// ===== GPT Model =====

class GPT {
    tok_emb: Module
    pos_emb: Module
    modules: List
    ln_f: Module
    lm_head: Module
    n_layers: Int

    def init(config: GPTConfig) {
        self.tok_emb = build_embedding(config.vocab_size, config.n_embd)
        self.pos_emb = build_embedding(config.block_size, config.n_embd)
        self.modules = List()
        self.n_layers = config.n_layer

        var i = 0
        while (i < config.n_layer) {
            self.modules.add(build_ln(config.n_embd))
            self.modules.add(build_proj(config.n_embd, config.n_embd))
            self.modules.add(build_proj(config.n_embd, config.d_head))
            self.modules.add(build_proj(config.n_embd, config.d_head))
            self.modules.add(build_proj(config.n_embd, config.n_embd))
            self.modules.add(build_ln(config.n_embd))
            self.modules.add(build_ffn(config.n_embd, config.d_ff))
            i = i + 1
        }

        self.ln_f = build_ln(config.n_embd)
        self.lm_head = build_proj(config.n_embd, config.vocab_size)
    }

    def to_device(device: String) {
        self.tok_emb.to_device(device)
        self.pos_emb.to_device(device)
        var i = 0
        while (i < self.modules.length()) {
            var m: Module = self.modules.get(i)
            m.to_device(device)
            i = i + 1
        }
        self.ln_f.to_device(device)
        self.lm_head.to_device(device)
    }

    def parameters() -> List {
        var params = List()
        params.add(self.tok_emb)
        params.add(self.pos_emb)
        var i = 0
        while (i < self.modules.length()) {
            params.add(self.modules.get(i))
            i = i + 1
        }
        params.add(self.ln_f)
        params.add(self.lm_head)
        return params
    }

    def forward(x: Tensor, causal_mask: Tensor) -> Tensor {
        var h: Tensor = self.tok_emb.forward(x) + self.pos_emb.forward(x)
        var bi = 0
        while (bi < self.n_layers) {
            var base = bi * 7
            h = block_forward(
                self.modules.get(base), self.modules.get(base + 1),
                self.modules.get(base + 2), self.modules.get(base + 3),
                self.modules.get(base + 4), self.modules.get(base + 5),
                self.modules.get(base + 6), h, causal_mask)
            bi = bi + 1
        }
        h = self.ln_f.forward(h)
        return self.lm_head.forward(h)
    }

    def free() {
        nn_free(self.tok_emb)
        nn_free(self.pos_emb)
        var i = 0
        while (i < self.modules.length()) {
            nn_free(self.modules.get(i))
            i = i + 1
        }
        nn_free(self.ln_f)
        nn_free(self.lm_head)
    }
}

// ===== Entry point =====

def main() {
    println("=== GPT-2 with Multi-Query Attention (Nex) ===")
    println("")

    // Hyperparameters
    n_epochs = 20
    lr = 0.001
    seed = 42
    save_path = "gpt2_mqa.pt"

    // CLI argument parsing
    argc = env_args_count()
    i = 1
    while (i < argc) {
        arg = env_args_get(i)
        if (arg == "--epochs") {
            i = i + 1
            n_epochs = parse_int(env_args_get(i))
        }
        if (arg == "--lr") {
            i = i + 1
            lr = parse_float(env_args_get(i))
        }
        if (arg == "--seed") {
            i = i + 1
            seed = parse_int(env_args_get(i))
        }
        if (arg == "--save") {
            i = i + 1
            save_path = env_args_get(i)
        }
        i = i + 1
    }

    torch_manual_seed(seed)

    device = "cpu"
    if (cuda_is_available() == 1) {
        device = "cuda"
    }

    // Config: n_embd=64, n_head=4, n_layer=4, vocab=32, block=32, d_head=16, d_ff=256
    var config = GPTConfig(64, 4, 4, 32, 32, 16, 256)

    println("Config:")
    print("  n_embd     = ")
    println(config.n_embd)
    print("  vocab_size = ")
    println(config.vocab_size)
    print("  n_layer    = ")
    println(config.n_layer)
    print("  n_head     = ")
    println(config.n_head)
    print("  d_head     = ")
    println(config.d_head)
    print("  d_ff       = ")
    println(config.d_ff)
    print("  epochs     = ")
    println(n_epochs)
    print("  lr         = ")
    println(lr)
    print("  device     = ")
    println(device)
    println("")

    // Build model
    println("Building model...")
    var model = GPT(config)
    model.to_device(device)

    // Create optimizers (one per module)
    var params = model.parameters()
    var optims = List()
    var pi = 0
    while (pi < params.length()) {
        optims.add(optim_adam(params.get(pi), lr))
        pi = pi + 1
    }
    println("  Adam optimizers initialized")

    // Causal mask
    var ones_sq: Tensor = tensor_ones_like(tensor_eye(config.block_size))
    var upper_tri: Tensor = ones_sq.triu(1)
    var causal_mask: Tensor = upper_tri.gt_scalar(0.5).to_device(device)

    // Training data
    var input_ids: Tensor = tensor_arange(0.0, 32.0, 1.0).to_device(device)
    var target: Tensor = tensor_to_dtype_long(tensor_arange(0.0, 32.0, 1.0)).to_device(device)

    println("")
    println("Training...")
    println("")

    // Training loop
    epoch = 0
    while (epoch < n_epochs) {
        var logits: Tensor = model.forward(input_ids, causal_mask)
        var loss: Tensor = logits.cross_entropy(target)
        var loss_val = loss.item()

        print("  epoch ")
        print(epoch)
        print("  loss = ")
        println(loss_val)

        loss.backward()

        for (opt in optims) {
            optim_step(opt)
            optim_zero_grad(opt)
        }

        epoch = epoch + 1
    }

    println("")
    println("Training complete!")

    // Inference
    println("")
    println("Running inference...")
    torch_no_grad(1)

    var test_in: Tensor = tensor_arange(0.0, 32.0, 1.0).to_device(device)
    var out: Tensor = model.forward(test_in, causal_mask)
    var preds: Tensor = out.argmax(1)

    print("  Output shape: [")
    print(out.shape_dim(0))
    print(", ")
    print(out.shape_dim(1))
    println("]")

    println("  Predictions (argmax, should be 0..31):")
    print("  ")
    preds.print()

    var preds_f: Tensor = preds.to_float()
    var target_f: Tensor = target.to_float()
    var diff: Tensor = (preds_f - target_f).abs()
    var correct: Tensor = diff.lt_scalar(0.5).to_float()
    var accuracy = correct.mean().item()
    print("  Accuracy: ")
    println(accuracy)

    torch_no_grad(0)

    // Save model
    model_save(model.lm_head, save_path)
    print("  Saved LM head weights to ")
    println(save_path)

    // Cleanup
    model.free()
    for (opt in optims) {
        optim_free(opt)
    }
    tensor_free(input_ids)
    tensor_free(target)
    tensor_free(causal_mask)
    tensor_free(test_in)

    println("")
    println("Done.")
    return
}
