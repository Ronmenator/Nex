// torch.tensor â€” Tensor creation, operations, and data access.

// ===========================================================================
// Device constants
// ===========================================================================

public def CPU() -> String  { return "cpu" }
public def CUDA() -> String { return "cuda" }

// ===========================================================================
// Tensor creation
// ===========================================================================

public def zeros(shape: Int64, ndims: Int64) -> Int64 {
    return tensor_zeros(shape, ndims)
}

public def ones(shape: Int64, ndims: Int64) -> Int64 {
    return tensor_ones(shape, ndims)
}

public def rand(shape: Int64, ndims: Int64) -> Int64 {
    return tensor_rand(shape, ndims)
}

public def randn(shape: Int64, ndims: Int64) -> Int64 {
    return tensor_randn(shape, ndims)
}

public def from_float_data(data: Int64, shape: Int64, ndims: Int64) -> Int64 {
    return tensor_from_float_data(data, shape, ndims)
}

public def arange(start: Float, end_val: Float, step: Float) -> Int64 {
    return tensor_arange(start, end_val, step)
}

public def eye(n: Int64) -> Int64 {
    return tensor_eye(n)
}

public def free_tensor(t: Int64) {
    tensor_free(t)
    return
}

// ===========================================================================
// Tensor operations
// ===========================================================================

public def add(a: Int64, b: Int64) -> Int64 {
    return tensor_add(a, b)
}

public def sub(a: Int64, b: Int64) -> Int64 {
    return tensor_sub(a, b)
}

public def mul(a: Int64, b: Int64) -> Int64 {
    return tensor_mul(a, b)
}

public def div(a: Int64, b: Int64) -> Int64 {
    return tensor_div(a, b)
}

public def matmul(a: Int64, b: Int64) -> Int64 {
    return tensor_matmul(a, b)
}

public def neg(t: Int64) -> Int64 {
    return tensor_neg(t)
}

public def exp(t: Int64) -> Int64 {
    return tensor_exp(t)
}

public def log(t: Int64) -> Int64 {
    return tensor_log(t)
}

public def sum(t: Int64) -> Int64 {
    return tensor_sum(t)
}

public def mean(t: Int64) -> Int64 {
    return tensor_mean(t)
}

// ===========================================================================
// Tensor shape
// ===========================================================================

public def reshape(t: Int64, shape: Int64, ndims: Int64) -> Int64 {
    return tensor_reshape(t, shape, ndims)
}

public def transpose(t: Int64, dim0: Int64, dim1: Int64) -> Int64 {
    return tensor_transpose(t, dim0, dim1)
}

public def squeeze(t: Int64) -> Int64 {
    return tensor_squeeze(t)
}

public def unsqueeze(t: Int64, dim: Int64) -> Int64 {
    return tensor_unsqueeze(t, dim)
}

public def shape_dim(t: Int64, dim: Int64) -> Int64 {
    return tensor_shape_dim(t, dim)
}

public def ndim(t: Int64) -> Int64 {
    return tensor_ndim(t)
}

public def numel(t: Int64) -> Int64 {
    return tensor_numel(t)
}

// ===========================================================================
// Tensor data access
// ===========================================================================

public def get_float(t: Int64, index: Int64) -> Float {
    return tensor_get_float(t, index)
}

public def item_float(t: Int64) -> Float {
    return tensor_item_float(t)
}

public def to_string(t: Int64) -> String {
    return tensor_to_string(t)
}

public def print(t: Int64) {
    tensor_print(t)
    return
}

// ===========================================================================
// Device management
// ===========================================================================

public def cuda_is_available() -> Bool {
    return cuda_is_available() != 0
}

public def cuda_device_count() -> Int64 {
    return cuda_device_count()
}

public def to_device(t: Int64, device: String) -> Int64 {
    return tensor_to_device(t, device)
}

public def set_num_threads(n: Int64) {
    set_num_threads(n)
    return
}

// ===========================================================================
// Autograd
// ===========================================================================

public def requires_grad(t: Int64, requires: Bool) -> Int64 {
    flag = 0
    if (requires) { flag = 1 }
    return tensor_requires_grad(t, flag)
}

public def backward(t: Int64) {
    tensor_backward(t)
    return
}

public def grad(t: Int64) -> Int64 {
    return tensor_grad(t)
}

public def no_grad(flag: Bool) {
    v = 0
    if (flag) { v = 1 }
    torch_no_grad(v)
    return
}

// ===========================================================================
// Utility
// ===========================================================================

public def manual_seed(seed: Int64) {
    torch_manual_seed(seed)
    return
}

public def version() -> String {
    return torch_version()
}

// ===========================================================================
// Scalar arithmetic
// ===========================================================================

public def add_scalar(t: Int64, scalar: Float) -> Int64 {
    return tensor_add_scalar(t, scalar)
}

public def mul_scalar(t: Int64, scalar: Float) -> Int64 {
    return tensor_mul_scalar(t, scalar)
}

public def div_scalar(t: Int64, scalar: Float) -> Int64 {
    return tensor_div_scalar(t, scalar)
}

public def pow_scalar(t: Int64, exponent: Float) -> Int64 {
    return tensor_pow_scalar(t, exponent)
}

// ===========================================================================
// Element-wise math (extended)
// ===========================================================================

public def sqrt(t: Int64) -> Int64 {
    return tensor_sqrt(t)
}

public def abs(t: Int64) -> Int64 {
    return tensor_abs(t)
}

public def clamp(t: Int64, min_val: Float, max_val: Float) -> Int64 {
    return tensor_clamp(t, min_val, max_val)
}

// Element-wise sine.
public def sin(t: Int64) -> Int64 {
    return tensor_sin(t)
}

// Element-wise cosine.
public def cos(t: Int64) -> Int64 {
    return tensor_cos(t)
}

// Applies softmax along the given dimension (tensor-level, not nn module).
public def softmax(t: Int64, dim: Int64) -> Int64 {
    return tensor_softmax(t, dim)
}

// ===========================================================================
// Comparison (return boolean/uint8 tensors)
// ===========================================================================

public def eq_scalar(t: Int64, value: Float) -> Int64 {
    return tensor_eq_scalar(t, value)
}

public def gt_scalar(t: Int64, value: Float) -> Int64 {
    return tensor_gt_scalar(t, value)
}

public def lt_scalar(t: Int64, value: Float) -> Int64 {
    return tensor_lt_scalar(t, value)
}

// ===========================================================================
// Masking and triangular
// ===========================================================================

// Returns the lower triangular part of a matrix. Elements above
// the given diagonal are set to zero.
public def tril(t: Int64, diagonal: Int64) -> Int64 {
    return tensor_tril(t, diagonal)
}

// Returns the upper triangular part of a matrix. Elements below
// the given diagonal are set to zero.
public def triu(t: Int64, diagonal: Int64) -> Int64 {
    return tensor_triu(t, diagonal)
}

// Fills elements of the tensor where mask is non-zero with value.
public def masked_fill(t: Int64, mask: Int64, value: Float) -> Int64 {
    return tensor_masked_fill(t, mask, value)
}

// Element-wise conditional: selects from x where condition is true, y otherwise.
public def where_self(condition: Int64, x: Int64, y: Int64) -> Int64 {
    return tensor_where_self(condition, x, y)
}

// ===========================================================================
// Reduction with dimension
// ===========================================================================

public def sum_dim(t: Int64, dim: Int64, keepdim: Int64) -> Int64 {
    return tensor_sum_dim(t, dim, keepdim)
}

public def mean_dim(t: Int64, dim: Int64, keepdim: Int64) -> Int64 {
    return tensor_mean_dim(t, dim, keepdim)
}

public def argmax(t: Int64, dim: Int64) -> Int64 {
    return tensor_argmax(t, dim)
}

public def max_dim(t: Int64, dim: Int64) -> Int64 {
    return tensor_max_dim(t, dim)
}

public def min_dim(t: Int64, dim: Int64) -> Int64 {
    return tensor_min_dim(t, dim)
}

// ===========================================================================
// Shape and indexing (extended)
// ===========================================================================

// Concatenates two tensors along the given dimension.
public def cat(a: Int64, b: Int64, dim: Int64) -> Int64 {
    return tensor_cat(a, b, dim)
}

// Returns a narrowed view: a slice of length elements starting at start
// along the given dimension.
public def narrow(t: Int64, dim: Int64, start: Int64, length: Int64) -> Int64 {
    return tensor_narrow(t, dim, start, length)
}

// Selects elements along dim using the given index tensor.
public def index_select(t: Int64, dim: Int64, index: Int64) -> Int64 {
    return tensor_index_select(t, dim, index)
}

// Flattens dimensions from start_dim to end_dim (inclusive).
public def flatten(t: Int64, start_dim: Int64, end_dim: Int64) -> Int64 {
    return tensor_flatten(t, start_dim, end_dim)
}

// ===========================================================================
// Creation helpers (extended)
// ===========================================================================

// Returns a tensor of ones with the same shape and device as the input.
public def ones_like(t: Int64) -> Int64 {
    return tensor_ones_like(t)
}

// Returns a tensor of zeros with the same shape and device as the input.
public def zeros_like(t: Int64) -> Int64 {
    return tensor_zeros_like(t)
}

// Returns a tensor filled with value, same shape and device as the input.
public def full_like(t: Int64, value: Float) -> Int64 {
    return tensor_full_like(t, value)
}

// ===========================================================================
// Utility (extended)
// ===========================================================================

// Returns a deep copy of the tensor.
public def clone(t: Int64) -> Int64 {
    return tensor_clone(t)
}

// Detaches the tensor from the computation graph.
public def detach(t: Int64) -> Int64 {
    return tensor_detach(t)
}

// Returns a contiguous tensor in memory.
public def contiguous(t: Int64) -> Int64 {
    return tensor_contiguous(t)
}

// Casts the tensor to float (f32/f64) dtype.
public def to_dtype_float(t: Int64) -> Int64 {
    return tensor_to_dtype_float(t)
}

// Casts the tensor to long (int64) dtype.
public def to_dtype_long(t: Int64) -> Int64 {
    return tensor_to_dtype_long(t)
}

// ===========================================================================
// Convenience functions
// ===========================================================================

// Returns "cuda" if a CUDA device is available, otherwise "cpu".
public def best_device() -> String {
    if (cuda_is_available() != 0) {
        return "cuda"
    }
    return "cpu"
}

// Returns true if the tensor is a scalar (0-dimensional).
public def is_scalar(t: Int64) -> Bool {
    return tensor_ndim(t) == 0
}

// Returns the total number of elements in the tensor (alias for numel).
public def total_elements(t: Int64) -> Int64 {
    return tensor_numel(t)
}

// Prints the shape of a tensor (supports up to 4 dimensions).
public def print_shape(t: Int64) {
    dims = tensor_ndim(t)
    if (dims == 0) {
        println("shape: []")
        return
    }
    if (dims == 1) {
        println("shape: [" + tensor_shape_dim(t, 0) + "]")
        return
    }
    if (dims == 2) {
        println("shape: [" + tensor_shape_dim(t, 0) + ", " + tensor_shape_dim(t, 1) + "]")
        return
    }
    if (dims == 3) {
        println("shape: [" + tensor_shape_dim(t, 0) + ", " + tensor_shape_dim(t, 1) + ", " + tensor_shape_dim(t, 2) + "]")
        return
    }
    if (dims == 4) {
        println("shape: [" + tensor_shape_dim(t, 0) + ", " + tensor_shape_dim(t, 1) + ", " + tensor_shape_dim(t, 2) + ", " + tensor_shape_dim(t, 3) + "]")
        return
    }
    println("shape: [" + dims + " dims]")
    return
}
