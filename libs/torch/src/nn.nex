// torch.nn — Neural network layers and modules.

// ===========================================================================
// Core layer functions (existing API — preserved)
// ===========================================================================

// Creates a new Sequential module for building neural networks.
public def sequential() -> Int64 {
    return nn_sequential_new()
}

// Adds a fully-connected (linear) layer to the module.
public def linear(module: Int64, in_features: Int64, out_features: Int64) {
    nn_linear(module, in_features, out_features)
    return
}

// Adds a 2D convolution layer to the module.
public def conv2d(module: Int64, in_channels: Int64, out_channels: Int64, kernel_size: Int64) {
    nn_conv2d(module, in_channels, out_channels, kernel_size)
    return
}

// Adds a ReLU activation layer.
public def relu(module: Int64) {
    nn_relu(module)
    return
}

// Adds a sigmoid activation layer.
public def sigmoid(module: Int64) {
    nn_sigmoid(module)
    return
}

// Adds a tanh activation layer.
public def tanh(module: Int64) {
    nn_tanh(module)
    return
}

// Adds a softmax activation layer along the given dimension.
public def softmax(module: Int64, dim: Int64) {
    nn_softmax(module, dim)
    return
}

// Adds a dropout layer with probability p.
public def dropout(module: Int64, p: Float) {
    nn_dropout(module, p)
    return
}

// Adds a 1D batch normalization layer.
public def batch_norm(module: Int64, features: Int64) {
    nn_batch_norm(module, features)
    return
}

// Runs a forward pass through the module.
public def forward(module: Int64, input: Int64) -> Int64 {
    return nn_forward(module, input)
}

// Frees a neural network module.
public def free(module: Int64) {
    nn_free(module)
    return
}

// ===========================================================================
// Combined layer builders — add a linear layer + activation in one call.
// ===========================================================================

public def linear_relu(module: Int64, in_f: Int64, out_f: Int64) {
    nn_linear(module, in_f, out_f)
    nn_relu(module)
    return
}

public def linear_sigmoid(module: Int64, in_f: Int64, out_f: Int64) {
    nn_linear(module, in_f, out_f)
    nn_sigmoid(module)
    return
}

public def linear_tanh(module: Int64, in_f: Int64, out_f: Int64) {
    nn_linear(module, in_f, out_f)
    nn_tanh(module)
    return
}

// ===========================================================================
// Model builders — create complete architectures in one call.
// ===========================================================================

// Builds a simple MLP: input -> hidden (ReLU) -> output.
public def build_mlp(in_features: Int64, hidden: Int64, out_features: Int64) -> Int64 {
    m = nn_sequential_new()
    nn_linear(m, in_features, hidden)
    nn_relu(m)
    nn_linear(m, hidden, out_features)
    return m
}

// Builds a classifier: input -> hidden (ReLU) -> num_classes (Softmax).
public def build_classifier(in_features: Int64, hidden: Int64, num_classes: Int64) -> Int64 {
    m = nn_sequential_new()
    nn_linear(m, in_features, hidden)
    nn_relu(m)
    nn_linear(m, hidden, num_classes)
    nn_softmax(m, 1)
    return m
}
